import sounddevice as sd
import numpy as np
import scipy.io.wavfile as wav
import whisper
import openai
import subprocess, os, time, tempfile

# Configurar TTS para aceitar licenÃ§a automaticamente
os.environ["COQUI_TOS_AGREED"] = "1"

# ImportaÃ§Ã£o para TTS
from TTS.api import TTS
from TTS.tts.configs.xtts_config import XttsConfig
from TTS.tts.models.xtts import Xtts
from TTS.utils.generic_utils import get_user_data_dir
from TTS.utils.manage import ModelManager

########################
# Konfiguration
########################
SAMPLERATE     = 16_000        # Hz
RECORD_SECS    = 4             # LÃ¤nge einer Aufnahme
LLM_MODEL      = "gpt-oss-1"   # Modelo personalizado
WHISPER_MODEL  = "base"        # tiny / base / small / medium â€¦
XTTS_MODEL     = "tts_models/multilingual/multi-dataset/xtts_v2"
XTTS_LANGUAGE  = "pt"          # PortuguÃªs
SYSTEM_PROMPT  = (
    "VocÃª Ã© um assistente Ãºtil. "
    "Interprete perguntas exclusivamente em portuguÃªs "
    "e responda sempre em portuguÃªs. "
    "NÃ£o use outros idiomas."
)

# Configurar o servidor personalizado da OpenAI
# Usando seu servidor que imita a API da OpenAI
openai.api_base = "https://gpt-proxy.ahvideoscdn.net/v1"
openai.api_key = "dummy-key"  # Chave dummy jÃ¡ que o servidor Ã© seu

########################
# TTS initialisieren (einmalig, nicht in der Schleife!)
########################
# Carregar modelo XTTS V2 usando a API simplificada (fallback)
print("ğŸ“¥ Carregando modelo XTTS V2 (API simplificada)...")
tts = TTS(
    model_name=XTTS_MODEL,
    progress_bar=False,
    gpu=False,
)
print("âœ… Modelo XTTS V2 carregado (API simplificada)!")

# Tentar preparar XTTS em modo low-level (como test_xtts_v2)
XTTS_LOWLEVEL_READY = False
xtts_model = None
try:
    print("ğŸ“¥ Preparando XTTS V2 (low-level)...")
    ModelManager().download_model(XTTS_MODEL)
    model_path = os.path.join(get_user_data_dir("tts"), XTTS_MODEL.replace("/", "--"))
    config = XttsConfig()
    config.load_json(os.path.join(model_path, "config.json"))
    xtts_model = Xtts.init_from_config(config)
    xtts_model.load_checkpoint(
        config,
        checkpoint_path=os.path.join(model_path, "model.pth"),
        vocab_path=os.path.join(model_path, "vocab.json"),
        eval=True,
        use_deepspeed=False,
    )
    # NÃ£o usar CUDA em ambiente local sem GPU
    XTTS_LOWLEVEL_READY = True
    print("âœ… XTTS V2 low-level pronto!")
except Exception as e:
    print(f"âš ï¸ XTTS low-level indisponÃ­vel, usando API simplificada. Motivo: {e}")


def say_text(text: str):
    """Gera Ã¡udio via XTTS. Usa low-level (inference_stream) quando disponÃ­vel, senÃ£o fallback para API simplificada."""
    try:
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
            tmp_path = tmp.name

        reference_audio = "reference_voice.wav"
        if not os.path.exists(reference_audio):
            print("ğŸ“ Criando arquivo de referÃªncia de voz...")
            create_reference_audio(reference_audio)

        if XTTS_LOWLEVEL_READY and xtts_model is not None:
            # Low-level: get_conditioning_latents + inference_stream
            print("ğŸµ [XTTS low-level] Processando referÃªncia...")
            gpt_cond_latent, speaker_embedding = xtts_model.get_conditioning_latents(audio_path=reference_audio)
            print("ğŸ¤ [XTTS low-level] Gerando Ã¡udio...")
            chunks = xtts_model.inference_stream(
                text,
                XTTS_LANGUAGE,
                gpt_cond_latent,
                speaker_embedding,
                repetition_penalty=2.5,
                temperature=0.6,
            )
            audio_chunks = []
            for chunk in chunks:
                chunk_np = chunk.detach().cpu().numpy().squeeze()
                # Converter para int16 para WAV
                chunk_i16 = (chunk_np * 32767).astype(np.int16)
                audio_chunks.append(chunk_i16)
            if audio_chunks:
                combined = np.concatenate(audio_chunks)
                wav.write(tmp_path, 24000, combined)
            else:
                raise RuntimeError("Nenhum Ã¡udio gerado pelo XTTS low-level")
        else:
            # Fallback: API simplificada
            print("ğŸ¤ [XTTS API] Gerando Ã¡udio...")
            tts.tts_to_file(
                text=text,
                language=XTTS_LANGUAGE,
                speaker=reference_audio,  # v2 aceita caminho WAV no speaker
                file_path=tmp_path,
                speed=0.8,
                temperature=0.6,
                repetition_penalty=2.5,
            )

        subprocess.run(["afplay", "-q", "1", tmp_path])
        os.remove(tmp_path)
        print(f"ğŸ”Š TTS: {text}")

    except Exception as e:
        print(f"âŒ TTS Erro: {e}")
        # Fallback para macOS say
        try:
            subprocess.run(["say", "-v", "Samantha", "-r", "160", text])
        except Exception:
            pass


def create_reference_audio(filename: str):
    """Cria um arquivo de Ã¡udio de referÃªncia simples"""
    try:
        subprocess.run([
            "say",
            "-v", "Samantha",
            "-o", filename,
            "OlÃ¡, esta Ã© minha voz de referÃªncia para o sistema de sÃ­ntese de fala."
        ], check=True)
        print(f"âœ… Arquivo de referÃªncia criado: {filename}")
    except Exception as e:
        print(f"âŒ Erro ao criar arquivo de referÃªncia: {e}")

########################
# STT- & LLM-Funktionen
########################
# Carregar modelo Whisper uma vez (global)
print("ğŸ“¥ Carregando modelo Whisper...")
import warnings
with warnings.catch_warnings():
    warnings.simplefilter("ignore")
    whisper_model = whisper.load_model(WHISPER_MODEL)
print("âœ… Modelo Whisper carregado!")

def record(seconds: int, sr: int = SAMPLERATE) -> str:
    print("ğŸ™ï¸  Fale agora...")
    audio = sd.rec(int(seconds * sr), samplerate=sr, channels=1, dtype='int16')
    sd.wait()
    fname = f"input_{int(time.time())}.wav"
    wav.write(fname, sr, audio)
    return fname

def speech_to_text(wav_path: str) -> str:
    # Usar modelo global carregado
    result = whisper_model.transcribe(wav_path, language="pt")
    return result["text"].strip()

def ask_llm(prompt: str) -> str:
    try:
        from openai import OpenAI
        client = OpenAI(
            api_key=openai.api_key,
            base_url=openai.api_base
        )
        response = client.chat.completions.create(
            model=LLM_MODEL,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=1000
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"âŒ Erro na API da OpenAI: {e}")
        return "Desculpe, houve um erro ao processar sua pergunta."

########################
# Haupt-Loop
########################
if __name__ == "__main__":
    print("ğŸ¤– Assistente de voz local iniciado â€“ Ctrl-C para sair")
    try:
        while True:
            wav_file = record(RECORD_SECS)
            question = speech_to_text(wav_file)
            os.remove(wav_file)
            if not question:
                continue
            print(f"ğŸ“ VocÃª disse: {question}")

            answer = ask_llm(question)
            print(f"ğŸ¤– Resposta: {answer}")

            say_text(answer)
    except KeyboardInterrupt:
        print("\nğŸ‘‹ AtÃ© logo!")

